## 대용량 트래픽을 견디는 서버를 위한 Layer 별 필수 CS 관련 개념

서비스를 운영하다 보면 예상치 못한 인기나 특정 이벤트로 갑작스럽게 많은 사용자가 몰리는 경우가 있습니다. 이런 상황에서 서버가 견고하게 버텨내지 못하면, 사용자 경험이 나빠지고 서비스의 신뢰성에도 큰 타격을 입게 됩니다. 이번 글에서는 대용량 트래픽에 효과적으로 대응하기 위해 꼭 알아야 할 **1. 운영체제**, **2. 데이터베이스**, **3. 아키텍처** 측면의 필수 CS 개념을 살펴보겠습니다.

## 1. 운영체제 관점: 동시성과 비동기 처리

대규모 트래픽을 처리하는 서버에서 가장 중요한 운영체제 개념 중 하나는 **동시성(Concurrency)**과 **비동기 처리(Async Processing)**입니다.

서버가 동시에 여러 사용자의 요청을 빠르게 처리하기 위해서는, Blocking/Non-blocking, Synchronous/Asynchronous의 차이를 명확히 이해해야 합니다.

- **Blocking**: 기다림이 반드시 필요 (예: 창구에서 줄서서 기다리기)
- **Non-blocking**: 기다리지 않고 바로 다른 일 수행 (예: 번호표 뽑고 대기, 카페에서 호출)
- **Synchronous**: 작업 순서를 꼭 지켜야 함 (예: 업무 처리 순서)
- **Asynchronous**: 순서를 꼭 지키지 않아도 됨, 결과가 오면 처리 (예: 맡겨놓고 나중에 연락받기)

### Blocking, Non-blocking, Synchronous, Asynchronous – 2x2 비교

요청 처리 방식은 크게 **Blocking/Non-blocking**과 **Synchronous/Asynchronous** 두 축으로 구분할 수 있습니다.

이 두 축을 결합하면 아래와 같이 2x2 표로 정리할 수 있습니다.

|  | **Synchronous(동기)** | **Asynchronous(비동기)** |
| --- | --- | --- |
| **Blocking** | 작업이 끝날 때까지 요청한 쪽이 계속 대기하며, 완료 후에만 다음 작업을 진행 | 요청을 맡긴 뒤, 결과가 준비될 때까지 아무것도 못 하고 기다림 |
| **Non-blocking** | 요청 후 바로 제어권이 돌아오지만, 결과를 기다리며 실제 다음 작업은 못 함 | 요청 후 곧장 다음 작업을 하고, 결과가 오면 콜백 등으로 별도 처리 |
- **Blocking/Synchronous**: 요청을 보낸 스레드가 응답이 올 때까지 아무 작업도 못 하고 대기합니다. (가장 직관적이지만 비효율적인 구조)
- **Non-blocking/Asynchronous**: 요청 후 대기하지 않고 바로 다음 작업을 수행, 결과가 준비되면 콜백 등으로 처리합니다. (고성능 서버에서 필수)

> 실제 예시
> 
> - Tomcat Thread Pool에서 HTTP 요청을 처리할 때는 일반적으로 **Blocking + Synchronous** 방식이 기본입니다.
> - Node.js의 이벤트 루프는 **Non-blocking + Asynchronous** 대표적 구조입니다.

### 동시성을 높이는 메커니즘 – Thread와 Event Loop

서버에서 동시성을 높이기 위해 대표적으로 두 가지 방식이 활용됩니다.

- **Thread(스레드) 방식**: 각 요청마다 별도의 스레드를 할당해 병렬로 처리합니다. 멀티스레딩으로 동시에 많은 요청을 처리할 수 있지만, 스레드 관리 오버헤드가 큽니다.
- **Event Loop 방식**: 하나의 스레드가 여러 요청을 관리하며, 작업 완료 이벤트를 감지해 효율적으로 처리합니다. Node.js가 대표적입니다.

### 예시: Spring Boot + Tomcat Thread Pool 관리

Spring Boot에서 기본으로 사용하는 Tomcat은 요청을 처리하기 위해 **Thread Pool**을 사용합니다.

Thread Pool은 여러 요청을 효율적으로 처리하고, 서버 리소스의 낭비를 방지합니다. 실무에서는 아래와 같은 설정을 튜닝하는 것이 중요합니다.

- **maxThreads**: 동시에 처리 가능한 최대 스레드 수
- **minSpareThreads**: 기본적으로 유지할 최소 스레드 수
- **acceptCount**: Thread Pool이 꽉 찼을 때 대기하는 큐의 최대 크기

이 값을 적절히 설정하지 않으면, 스레드 고갈로 인한 응답 지연 또는 서버 다운 등 심각한 문제가 발생할 수 있습니다.

운영체제 수준의 이해를 바탕으로, 실서비스에서 성능 튜닝 및 관리가 반드시 필요합니다.

## 2. 데이터베이스 관점: 트랜잭션 격리 수준과 커넥션 풀링

대용량 트래픽 환경에서 데이터베이스는 서버의 병목 지점이 되기 쉽습니다. 서버에서 아무리 많은 요청을 동시에 받아도, 데이터베이스 연결이 충분히 준비되어 있지 않거나, 동시성 처리에 미숙하다면 서비스 전체가 느려질 수밖에 없습니다.

### 트랜잭션 격리 수준(Transaction Isolation Level)

트랜잭션은 데이터베이스의 **일관성**과 **정합성**을 보장하는 핵심 개념입니다. 동시에 여러 사용자가 같은 데이터를 읽고 쓸 때, 어떤 트랜잭션이 먼저 적용될지, 데이터가 어떤 시점에 보일지 등을 제어하는 것이 바로 **격리 수준**입니다.

- 높은 격리 수준(Serializable 등)은 데이터의 정합성을 최대한 보장하지만, 동시 처리 성능이 떨어질 수 있습니다.
- 낮은 격리 수준(Read Uncommitted 등)은 동시 처리는 빨라지지만, 잘못된 데이터가 보일 수 있는 위험이 있습니다.

| 격리 수준 | Dirty Read | Non-repeatable Read | Phantom Read | 성능 |
| --- | --- | --- | --- | --- |
| Read Uncommitted | O | O | O | 빠름 |
| Read Committed | X | O | O | 보통 |
| Repeatable Read | X | X | O | 느림 |
| Serializable | X | X | X | 가장 느림 |

**비유:**

격리 수준은 마치 도서관에서 책을 빌리는 규칙과도 같습니다.

- 가장 엄격한 규칙(Serializable)은 한 사람이 책을 보는 동안 누구도 그 책에 손을 댈 수 없습니다.
- 가장 느슨한 규칙(Read Uncommitted)은 누군가가 아직 반납도 안 한 책을 다른 사람이 몰래 읽는 것과 비슷합니다.

따라서, 대용량 트래픽 상황에서는 **성능과 정합성 사이에서 적절한 타협점**을 찾는 것이 중요합니다. 예를 들어, 금융 서비스는 정합성을 우선하지만, 단순 조회 서비스는 성능을 더 중시하기도 합니다.

### 커넥션 풀링(Connection Pooling)

트래픽이 많아질수록 데이터베이스 연결을 효율적으로 관리하는 것이 중요합니다.

**커넥션 풀**은 미리 여러 개의 DB 연결을 만들어두고, 요청이 들어올 때마다 이 연결을 “대여”하는 방식입니다. 작업이 끝나면 다시 풀로 반환하여 재사용합니다.

- 새로운 연결을 그때그때 만드는 것에 비해, **속도가 훨씬 빠르고** 데이터베이스 자원을 효율적으로 관리할 수 있습니다.
- 실무에서는 HikariCP, Tomcat JDBC Pool 등 다양한 커넥션 풀 라이브러리가 표준처럼 사용됩니다.

| 관리 방식 | 특징 | 실생활 비유 |
| --- | --- | --- |
| 풀링 사용 X | 요청마다 새 연결 생성 | 매번 식당에서 새 의자를 만드는 것 |
| 커넥션 풀링 | 미리 여러 연결을 준비, 요청마다 대여 | 손님이 올 때마다 준비된 의자에 앉히는 것 |

커넥션 풀이 없으면 대기열이 길어져 서버 전체가 느려지며, 반대로 풀의 크기가 너무 크면 DB 서버가 과부하될 수 있으니, **적절한 풀 사이즈 튜닝이 중요**합니다.

> 핵심:
> 
> 
> 대용량 트래픽 환경에서 데이터베이스는 가장 약한 고리가 되기 쉽습니다.
> 
> “내 서비스의 특성과 트래픽 패턴에 맞는 격리 수준과 커넥션 풀 설정”이
> 
> 곧 서버 전체의 견고함과 직결된다는 점을 꼭 기억해야 합니다.
> 

## 3. 아키텍처 관점: 로드 밸런싱과 수평 확장성

대용량 트래픽 환경에서 서버 아키텍처는 “얼마나 효율적으로 트래픽을 분산하고, 필요에 따라 빠르게 확장할 수 있는가”가 핵심입니다. 이때 반드시 이해해야 할 두 가지 키워드가 바로 **로드 밸런싱(Load Balancing)**과 **수평 확장성(Horizontal Scalability)**입니다.

### 수평 확장성(Horizontal Scalability)

수평 확장성은 **서버의 개수를 늘려 성능을 확장하는 방식**입니다.

마치 손님이 많아질 때 계산대를 추가로 여는 것과 같은 개념입니다.

- **수직 확장(Vertical Scaling)**: 한 대의 서버의 CPU, 메모리 등 자원을 늘리는 것
- **수평 확장(Horizontal Scaling)**: 서버의 수를 늘려 여러 대로 분산하는 것

| 방식 | 특징 | 장단점 |
| --- | --- | --- |
| 수직 확장 | 한 서버를 점점 더 강력하게 업그레이드 | 간단하지만 확장에 한계 |
| 수평 확장 | 서버를 여러 대로 나누고 부하를 분산 | 무한에 가깝게 확장, 관리 복잡 |

실무에서는 Kubernetes와 같은 **컨테이너 오케스트레이션 플랫폼**이나, AWS ECS/EKS 같은 클라우드 서비스를 활용해 트래픽 변화에 따라 서버를 자동으로 늘리거나 줄일 수 있습니다.

### 로드 밸런싱(Load Balancing)

로드 밸런싱은 들어오는 수많은 요청을 여러 서버에 **고르게 분산**시키는 기술입니다.

이는 마치 푸드코트에서 손님들이 한 곳에 몰리지 않도록, 직원이 손님을 빈 자리에 안내하는 것과 비슷합니다.

- **왜 필요한가?**
    
    한 대의 서버만 이용하면 트래픽이 집중될 때 쉽게 과부하가 발생합니다. 여러 대의 서버로 요청을 나누면 장애에 강하고 응답 속도도 빨라집니다.
    
- **분산 알고리즘의 종류**
    
    
    | 알고리즘 | 특징 |
    | --- | --- |
    | Round Robin | 순서대로 서버에 요청 분배 |
    | Least Connections | 현재 연결이 가장 적은 서버에 우선 분배 |
    | IP Hash | 같은 IP의 요청을 항상 같은 서버로 전달 |
- **실무 적용 예시**
    - Nginx, HAProxy, AWS ELB(Elastic Load Balancer) 등 다양한 소프트웨어/클라우드 솔루션이 표준으로 사용됩니다.
    - Spring Boot 기반의 서비스 역시 LB 뒤에 여러 대가 배치되어 운영되는 것이 일반적입니다.

## 결론

### 결론

대용량 트래픽을 견디는 견고한 서버를 만들기 위해서는, 서비스의 여러 **Layer**(운영체제, 데이터베이스, 아키텍처 등)에 걸쳐 발생할 수 있는 병목 지점을 정확하게 식별하고 해결하는 일이 무엇보다 중요합니다.

이러한 작업을 효과적으로 수행하기 위해서는 각 Layer에 대한 **컴퓨터 공학(CS) 기반의 탄탄한 이해**가 반드시 필요합니다.

운영체제의 동시성, 데이터베이스의 트랜잭션 격리와 커넥션 풀링, 그리고 아키텍처의 로드 밸런싱과 확장성 등, 이론적 지식이 실무에서 어떻게 연결되는지 이해해야 비로소 복잡한 장애 상황에서도 흔들리지 않는 선택을 할 수 있습니다.

이번 글이, 각 Layer에서 발생할 수 있는 병목 현상을 해결하기 위한 CS 개념 학습에 작은 길잡이가 되었기를 바랍니다.